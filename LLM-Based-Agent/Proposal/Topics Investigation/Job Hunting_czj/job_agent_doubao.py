# -*- coding: utf-8 -*-
"""
Job Seeking Agent (Version 5.0 - ç«å±±æ–¹èˆŸè±†åŒ…APIç‰ˆ)
é€‚é…ç«å±±æ–¹èˆŸå¹³å°è±†åŒ…APIï¼Œå®ç°èŒä½æœç´¢ã€ç®€å†ä¼˜åŒ–å’Œèƒ½åŠ›åˆ†æ
"""

import os
import textwrap
from openai import OpenAI
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# ç«å±±æ–¹èˆŸAPIé…ç½®
ARK_API_KEY = os.getenv("ARK_API_KEY")
if not ARK_API_KEY:
    raise ValueError("è¯·è®¾ç½® 'ARK_API_KEY' ç¯å¢ƒå˜é‡")

client = OpenAI(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=ARK_API_KEY
)

def call_ark_doubao_api(prompt, model="doubao-seed-1-6-250615", temperature=0.7):
    """è°ƒç”¨ç«å±±æ–¹èˆŸè±†åŒ…æ¨¡å‹ç”Ÿæˆå†…å®¹"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=2048
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"è°ƒç”¨å¤±è´¥: {str(e)}"


class JobSeekingAgent:
    def __init__(self):
        self.resume_text = ""
        self.job_postings = []
        print("ğŸ¤– æ±‚èŒAgentå·²å¯åŠ¨")

    def load_resume(self, resume_path="/Users/zijiancai/Desktop/hkucs files/comp7607/find_job_agent/resume.txt"):
        try:
            with open(resume_path, 'r', encoding='utf-8') as f:
                self.resume_text = f.read()
            print("ğŸ“„ ç®€å†åŠ è½½æˆåŠŸ")
        except FileNotFoundError:
            print(f"é”™è¯¯: ç®€å†æ–‡ä»¶ {resume_path} æœªæ‰¾åˆ°")
            exit()

    def tool_search_jobs(self, query: str, num_jobs: int = 5) -> list:
        query = query.replace(' ', '+')
        url = f"https://weworkremotely.com/remote-jobs/search?term={query}"
        print(f"\nğŸ” æ­£åœ¨æœç´¢ '{query}' èŒä½...")

        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/108.0.0.0 Safari/537.36")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--disable-blink-features=AutomationControlled")

        driver = None
        try:
            service = ChromeService(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.get(url)
            wait = WebDriverWait(driver, 15)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "section.jobs li")))
            page_html = driver.page_source
            soup = BeautifulSoup(page_html, 'html.parser')

        except TimeoutException:
            print("âŒ é¡µé¢åŠ è½½è¶…æ—¶")
            if driver:
                driver.save_screenshot("debug_screenshot.png")
                with open("debug_page_source.html", "w", encoding='utf-8') as f:
                    f.write(driver.page_source)
                print("â„¹ï¸ å·²ä¿å­˜è°ƒè¯•ä¿¡æ¯")
                driver.quit()
            return []
        except Exception as e:
            print(f"âŒ Selenium é”™è¯¯: {e}")
            if driver:
                driver.quit()
            return []

        scraped_jobs = []
        jobs_list_container = soup.find('section', class_='jobs')
        if jobs_list_container:
            job_items = jobs_list_container.select('li.new-listing-container')
            for item in job_items[:num_jobs]:
                title_element = item.select_one('h3.new-listing__header__title')
                company_element = item.select_one('p.new-listing__company-name')
                link_element = item.select_one('a[href*="/remote-jobs/"]')
                if title_element and company_element and link_element:
                    title = title_element.text.strip()
                    company = company_element.text.strip()
                    job_link = "https://weworkremotely.com" + link_element['href']
                    scraped_jobs.append({
                        "title": title,
                        "company": company,
                        "description": f"èŒä½: {title}\nå…¬å¸: {company}\nè¯¦æƒ…: {job_link}"
                    })
        
        if not scraped_jobs:
            print("æœªè§£æå‡ºèŒä½ä¿¡æ¯ï¼Œå¯èƒ½éœ€è¦æ›´æ–°CSSé€‰æ‹©å™¨")
        else:
            print(f"âœ… æ‰¾åˆ° {len(scraped_jobs)} ä¸ªèŒä½")
            
        self.job_postings = scraped_jobs
        driver.quit()
        return scraped_jobs
        
    def tool_optimize_resume(self, job_description: str) -> str:
        print("\nâœ¨ ç”Ÿæˆç®€å†ä¼˜åŒ–å»ºè®®...")
        prompt = f"""
        ä½ æ˜¯ITèŒä¸šè§„åˆ’å¸ˆï¼Œè¯·æŒ‰å²—ä½æè¿°ä¼˜åŒ–ç®€å†ï¼š
        1. ç²¾å‡†åŒ¹é…æŠ€æœ¯å…³é”®è¯
        2. æˆæœå¯¼å‘ï¼ˆç”¨æ•°å­—é‡åŒ–ï¼‰
        3. çªå‡ºç›¸å…³äº®ç‚¹
        4. ç›´æ¥è¾“å‡ºä¼˜åŒ–åç®€å†

        ---
        å²—ä½æè¿°:
        {job_description}
        ---
        åŸå§‹ç®€å†:
        {self.resume_text}
        ---
        """
        return call_ark_doubao_api(prompt)

    def tool_supplement_materials(self, job_description: str) -> str:
        print("\nğŸ”¬ åˆ†ææŠ€èƒ½çŸ­æ¿...")
        prompt = f"""
        ä½ æ˜¯è½¯ä»¶å·¥ç¨‹å¸ˆé¢è¯•å®˜ï¼Œè¯·ç”Ÿæˆèƒ½åŠ›æå‡æŠ¥å‘Šï¼š
        1. æŠ€èƒ½å·®è·åˆ†æ
        2. çŸ¥è¯†å¼ºåŒ–è·¯å¾„
        3. æ ¸å¿ƒé¢è¯•é¢˜é¢„æµ‹

        ---
        å²—ä½è¦æ±‚:
        {job_description}
        ---
        æ±‚èŒè€…ç®€å†:
        {self.resume_text}
        ---
        """
        return call_ark_doubao_api(prompt)

    def run(self, job_query: str):
        self.load_resume()
        self.tool_search_jobs(job_query)
        if not self.job_postings:
            print("æœªæ‰¾åˆ°èŒä½ï¼Œç¨‹åºé€€å‡º")
            return

        target_job = self.job_postings[0]
        print(f"\nğŸ¯ ç›®æ ‡èŒä½ï¼šã€{target_job['title']}ã€‘ at ã€{target_job['company']}ã€‘")
        print("-------------------- èŒä½æè¿° --------------------")
        print(textwrap.fill(target_job['description'], width=80))
        print("--------------------------------------------------\n")

        optimized_resume = self.tool_optimize_resume(target_job['description'])
        print("\n\n=============== ä¼˜åŒ–åçš„ç®€å† ================")
        print(optimized_resume)
        print("============================================\n\n")

        gap_analysis_report = self.tool_supplement_materials(target_job['description'])
        print("=============== èƒ½åŠ›æå‡æŠ¥å‘Š ================")
        print(gap_analysis_report)
        print("=============================================")

if __name__ == '__main__':
    agent = JobSeekingAgent()
    agent.run("Python Developer")